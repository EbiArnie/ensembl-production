h1. GTF Pipeline

This is a re-implementation of an existing pipeline developed originally by core and the webteam. The new version uses eHive, so familiarity with this system is essential, and has been written to use as little memory as possible.

h2. The Registry File

This is the way we retrieve the database connections to work with. The registry file should specify:

* The core (and any other) databases to dump from

Here is an example of a registry file used for the first run of the pipeline (Ensembl v72). Note the use of the Registry object within a registry file and the scoping of the package. If you omit the *-db_version* parameter and only use HEAD checkouts of Ensembl then this will automatically select the latest version of the API. Any change to version here must be reflected in the configuration file.

bc. 
	package Reg;
	use Bio::EnsEMBL::Registry;
	Bio::EnsEMBL::Registry->no_version_check(1);
	Bio::EnsEMBL::Registry->no_cache_warnings(1);
	{
	  my $version = $ENV{RELEASE};
	  my $user = 'ensro';
	  my $host = 'ensdb-5-g-07';
	  my $port = 5307;
	  Bio::EnsEMBL::Registry->load_registry_from_multiple_dbs(
	    {
	      -host => $host,
	      -port => $port,
	      -db_version => $version,
	      -user => "user",
	      -NO_CACHE => 1,
	    }
	  );
	}
	1;

You give the registry to the *init_pipeline.pl* script via the *-registry* option

h2. Overriding Defaults Using a New Config File 

We recommend if you have a number of parameters which do not change between releases to create a configuration file which inherits from the root config file e.g.

bc. 
	package MyCnf;
	use base qw/Bio::EnsEMBL::Production::Pipeline::PipeConfig::GTF_conf/;
	sub default_options {
	  my ($self) = @_;
	  return {
	    %{ $self->SUPER::default_options() },
	    #Override of options
	  };
	}
	1;

If you do override the config then you should use the package name for your overridden config in the upcoming example commands.

h2. Environment

h3. PERL5LIB

* ensembl-production
* ensembl
* ensembl-hive
* bioperl

h3. PATH

* ensembl-hive/scripts
* gtfToGenePred
* genePredCheck

h3. ENSEMBL_CVS_ROOT_DIR

Set to the base checkout of Ensembl. We should be able to add *ensembl-hive/sql* onto this path to find the SQL directory for hive e.g.

bc. 
	export ENSEMBL_CVS_ROOT_DIR=$HOME/work/ensembl-checkouts

h3. ENSADMIN_PSW

Give the password to use to log into a database server e.g.

bc. 
	export ENSADMIN_PSW=wibble

h2. Command Line Arguments

Where *Multiple Supported* is supported we allow the user to specify the parameter more than once on the command line. For example species is one of these options e.g. 

bc. -species human -species cele -species yeast

|_. Name |_.  Type|_. Multiple Supported|_.  Description|_. Default|_.  Required|
|@-registry@|String|No|Location of the Ensembl registry to use with this pipeline|-|*YES*|
|@-base_path@|String|No|Location of the dumps|-|*YES*|
|@-pipeline_db -host=@|String|No|Specify a host for the hive database e.g. @-pipeline_db -host=myserver.mysql@|See hive generic config|*YES*|
|@-pipeline_db -dbname=@|String|No|Specify a different database to use as the hive DB e.g. @-pipeline_db -dbname=my_dumps_test@|Uses pipeline name by default|*NO*|
|@-species@|String|Yes|Specify one or more species to process. Pipeline will only _consider_ these species|-|*NO*|
|@-pipeline_name@|String|No|Name to use for the pipeline|$USER_gtf_dump_$release|*NO*|
|@-release@|Integer|No|The release to dump|Software version|*NO*|
|@-email@|String|No|Email to send pipeline summaries to upon its successful completion|$USER@sanger.ac.uk|*NO*|

h2. Example Commands

h3. To load use normally:

bc. 
	init_pipeline.pl Bio::EnsEMBL::Production::Pipeline::PipeConfig::GTF_conf \
	-pipeline_db -host=my-db-host -base_path /path/to/dumps -registry reg.pm

h3. Run a subset of species (no forcing & supports registry aliases):

bc. 
	init_pipeline.pl Bio::EnsEMBL::Production::Pipeline::PipeConfig::GTF_conf \
	-pipeline_db -host=my-db-host -species anolis -species celegans -species human \
	-base_path /path/to/dumps -registry reg.pm


h2. Running the Pipeline

# Start a screen session or get ready to run the beekeeper with a @nohup@
# Choose a dump location
#* A gtf directory will be created 1 level below
# Use an @init_pipeline.pl@ configuration from above
#* Make sure to give it the @-base_path@ parameter
# Sync the database using one of the displayed from @init_pipeline.pl@
# Run the pipeline in a loop with a good sleep between submissions and redirect log output (the following assumes you are using *bash*)
#* @2>&1@ is important as this clobbers STDERR into STDOUT
#* @> my_run.log@ then sends the output to this file. Use @tail -f@ to track the pipeline
#	@beekeeper.pl -url mysql://usr:pass@server:port/db -reg_conf reg.pm -loop -sleep 5 2>&1 > my_run.log &@
# Wait

h2. Running without a Pipeline

Hive gives us the ability to run any Process outside of a database pipeline 
run using @standaloneJob.pl@. We will list some useful commands to run

h3. Dumping a Single Species

bc. 
  standaloneJob.pl Bio::EnsEMBL::Production::Pipeline::GTF::DumpFile \
  -reg_conf reg.pm -debug 2 \
  -release 72 -species homo_sapiens \
  -base_path /path/to/dumps

h2. Full example

h3. Set up

bc. 
    $ export RELEASE=72
    $ export ENSEMBL_CVS_ROOT_DIR=$HOME/src
    $ export ENSADMIN_PSW=wibble
    $ export LOG_DIR=${ENSEMBL_CVS_ROOT_DIR}/ensembl-personal/${USER}/pipelines/${RELEASE}/gtf
    $ export DUMPS_DIR=/lustre/scratch109/ensembl/${USER}/${RELEASE}
    $ export PATH=$ENSEMBL_CVS_ROOT_DIR/ensembl-hive/scripts:/path/to/gtfToGenePred:$PATH
    $ cd $ENSEMBL_CVS_ROOT_DIR/ensembl-hive
    $ cvs -q up -dPA
    $ cvs -q up -dP -r stable
    $ mkdir -p $DUMPS_DIR

h3. Pipeline Init

bc. 
    $ cd $ENSEMBL_CVS_ROOT_DIR/ensembl
    $ init_pipeline.pl Bio::EnsEMBL::Production::Pipeline::PipeConfig::GTF_conf \
      -pipeline_db -host=ens-production -base_path ${DUMPS_DIR} -registry ${LOG_DIR}/registry.pm
    $ export HIVE_URL=mysql://username:password@ens-production:3306/${USER}_gtf_dump_${RELEASE}
    $ beekeeper.pl -url $HIVE_URL -sync

h3. Running

bc. 
    $ beekeeper.pl -url ${HIVE_URL} -reg_conf ${LOG_DIR}/registry.pm -loop -sleep 5 2>&1 > ${LOG_DIR}/dumps.hive.log  

